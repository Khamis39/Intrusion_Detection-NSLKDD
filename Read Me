However, itâ€™s important to note that this Siamese network model is an unsupervised learning model that   embed data points into a lower-dimensional space such that similar data points are close together and dissimilar data points are far apart. As such, traditional classification metrics like accuracy, precision, recall, and F1 score are not applicable to this type of model.
Instead, we use other methods to evaluate the performance of the model, such as visualizing the embeddings using dimensionality reduction techniques like t-SNE or UMAP. Here we created a 2D visualization of the embeddings using t-SNE:

t-SNE (t-Distributed Stochastic Neighbor Embedding) is a nonlinear dimensionality reduction technique that is particularly effective at preserving the local structure of the data. It works by minimizing the divergence between two distributions: a distribution that measures pairwise similarities of the input objects and a distribution that measures pairwise similarities of the corresponding low-dimensional points in the embedding1.

This code computes the embeddings for the test set using the predict method of the embedding_model, then reduces the dimensionality of the embeddings to 2D using t-SNE. Finally, it creates a scatter plot of the 2D embeddings, where each point is colored according to its class label.
scatter plot shows the 2D visualization of the embeddings of our data points using the t-SNE technique. The plot has four distinct clusters of data points, each represented by a different color. The color scale on the right side of the plot ranges from 1 to 10, indicating that there are 10 different classes in our data.

The t-SNE technique is designed to preserve the local structure of the data, so data points that are close together in the high-dimensional space are likely to be close together in the 2D visualization as well. From the plot, we can see that the four clusters are well-separated from each other, indicating that the Siamese network model is able to learn a good representation of the data that separates the different classes.

You will also code find the code snippet that defines a load_and_preprocess_dataset function that takes as input the path to a CSV file containing the dataset and an optional n_components parameter specifying the number of features to select. The function loads the dataset, preprocesses it by scaling the features and encoding any categorical variables, applies PCA to reduce its dimensionality, and visualizes the resulting PCA using a scatter plot. The function then selects the top n_components features based on their contribution to the first principal component and returns these selected features along with the labels.
