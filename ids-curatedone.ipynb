{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mFailed to start the Kernel. \n","\u001b[1;31mUnable to start Kernel 'Python 3.9.13' due to connection timeout. \n","\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-08-10T12:16:30.285043Z","iopub.status.busy":"2023-08-10T12:16:30.284621Z","iopub.status.idle":"2023-08-10T12:16:39.472583Z","shell.execute_reply":"2023-08-10T12:16:39.471381Z","shell.execute_reply.started":"2023-08-10T12:16:30.285007Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder,StandardScaler\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","%matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-10T12:20:38.636850Z","iopub.status.busy":"2023-08-10T12:20:38.636379Z","iopub.status.idle":"2023-08-10T12:20:38.645515Z","shell.execute_reply":"2023-08-10T12:20:38.644193Z","shell.execute_reply.started":"2023-08-10T12:20:38.636812Z"},"trusted":true},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mFailed to start the Kernel. \n","\u001b[1;31mUnable to start Kernel 'Python 3.9.13' due to connection timeout. \n","\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["import os\n","NID_dataset_json={'Name':'Network Intrusion Detection',\n","                  'path_train_csv':'/kaggle/input/network-intrusion-detection/Train_data.csv',\n","                  'path_test_csv': '/kaggle/input/network-intrusion-detection/Test_data.csv',\n","                  'target_field':'class'\n","                 }\n","\n","NSLKDD_dataset_json={'title':'NSL KDD',\n","                     'path_train_csv':'/conndataset/Conn.csv',\n","                     'path_test_csv': '/kaggle/input/nsl-dataset/test500.csv',\n","                     'target_field':'class'\n","                    }\n","NIDS_dataset_json={'title':'NIDS V2',\n","                     'path_train_csv':'/kaggle/input/nfuqnidsv2-network-intrusion-detection-dataset/NF-UQ-NIDS-v2.csv',\n","                     'target_field':'class'\n","                    }\n","\n","hyperparameter={'epoch':100,\n","                'batch-size':32,\n","                'loss-function':'triplet_loss'\n","               }\n","\n","\n","os.mkdir('model') if not os.path.isdir('model') else None\n","os.mkdir('model/checkpoint') if not os.path.isdir('model/checkpoint') else None\n","\n","dataset_json=NSLKDD_dataset_json"]},{"cell_type":"markdown","metadata":{},"source":["This code defines a load_and_preprocess_dataset function that takes as input the path to a CSV file containing the dataset and an optional n_components parameter specifying the number of features to select. The function loads the dataset, preprocesses it by scaling the features and encoding any categorical variables, applies PCA to reduce its dimensionality, and visualizes the resulting PCA using a scatter plot. The function then selects the top n_components features based on their contribution to the first principal component and returns these selected features along with the labels."]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-08-10T12:20:46.150836Z","iopub.status.busy":"2023-08-10T12:20:46.150428Z","iopub.status.idle":"2023-08-10T12:20:47.073093Z","shell.execute_reply":"2023-08-10T12:20:47.071893Z","shell.execute_reply.started":"2023-08-10T12:20:46.150802Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","\n","# Load the data from the CSV or Excel file\n","data = pd.read_csv(dataset_json['path_train_csv']) # or pd.read_excel('data.xlsx')\n","\n","# Calculate the number of rows to select\n","n_rows = int(len(data) / 2)\n","\n","# Select the first half of the data\n","data_first_half = data.iloc[:n_rows, :]\n","\n","# Select the second half of the data\n","data_second_half = data.iloc[n_rows:, :]\n","\n","# Preprocess the selected data\n","# ...\n"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-08-10T12:20:54.338631Z","iopub.status.busy":"2023-08-10T12:20:54.338221Z","iopub.status.idle":"2023-08-10T12:20:54.348365Z","shell.execute_reply":"2023-08-10T12:20:54.347245Z","shell.execute_reply.started":"2023-08-10T12:20:54.338598Z"},"trusted":true},"outputs":[],"source":["from sklearn.preprocessing import LabelEncoder\n","\n","def load_and_preprocess_dataset(path_csv, n_components=14):\n","    df = data_first_half#pd.read_csv(path_csv)\n","    \n","    # Drop columns with zero variance\n","    zero_variance_columns = ['missed_bytes']\n","    #zero_variance_columns = ['is_host_login\"', 'num_outbound_cmds', 'num_shells', 'root_shell', 'urgent', 'land']\n","    df.drop(columns=zero_variance_columns, inplace=True)\n","    \n","    # Split into features and labels\n","    X = df.iloc[:, :-1]\n","    y = df.iloc[:, -1]\n","    \n","    # Encode categorical features using one-hot encoding\n","    categorical_columns = X.select_dtypes(include=['object']).columns.tolist()\n","    X = pd.get_dummies(data=X, columns=categorical_columns)\n","    \n","    scaler = StandardScaler()\n","    X_scaled = scaler.fit_transform(X)\n","\n","    # Create an instance of the PCA class\n","    pca = PCA(n_components=n_components)\n","\n","    # Fit the PCA model to the data\n","    X_pca = pca.fit_transform(X_scaled)\n","\n","    # Encode labels using label encoding\n","    label_encoder = LabelEncoder().fit(y)\n","    y_encoded = label_encoder.transform(y)\n","    \n","    # Visualize the resulting PCA\n","    plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_encoded)\n","    plt.xlabel('First Principal Component')\n","    plt.ylabel('Second Principal Component')\n","    plt.show()\n","\n","    return X_pca, y_encoded\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import tensorflow as tf\n","\n","\n","def preprocess_data(dataset_path):\n","    # Load the data from the CSV file\n","    dataset = tf.data.experimental.CsvDataset(\n","        dataset_path,\n","        record_defaults, # define the data types for each column\n","        header=True,\n","        field_delim=','\n","    )\n","\n","    # Apply preprocessing operations to the data\n","    dataset = dataset.map(preprocess_function) # define a function to preprocess the data\n","\n","    # Batch the data\n","    dataset = dataset.batch(batch_size)\n","\n","    return dataset\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-07T11:34:30.055530Z","iopub.status.busy":"2023-08-07T11:34:30.055103Z","iopub.status.idle":"2023-08-07T11:34:30.141977Z","shell.execute_reply":"2023-08-07T11:34:30.140072Z","shell.execute_reply.started":"2023-08-07T11:34:30.055495Z"},"trusted":true},"outputs":[],"source":["# Load and preprocess dataset\n","X,y=load_and_preprocess_dataset(dataset_json['path_train_csv'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-10T12:20:58.881920Z","iopub.status.busy":"2023-08-10T12:20:58.881493Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_32/1244496347.py:9: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df.drop(columns=zero_variance_columns, inplace=True)\n"]}],"source":["X,y=load_and_preprocess_dataset(data_first_half)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-07T11:34:34.612868Z","iopub.status.busy":"2023-08-07T11:34:34.612444Z","iopub.status.idle":"2023-08-07T11:42:09.923694Z","shell.execute_reply":"2023-08-07T11:42:09.922467Z","shell.execute_reply.started":"2023-08-07T11:34:34.612826Z"},"trusted":true},"outputs":[],"source":["\n","\n","def create_triplet_samples(x_train, y_train, n_features, n_batch):\n","    x_anchors = np.zeros((n_batch, n_features))\n","    x_positives = np.zeros((n_batch, n_features))\n","    x_negatives = np.zeros((n_batch, n_features))\n","    \n","    for i in range(n_batch):\n","        # Select a random sample from x_train as the anchor\n","        anchor_idx = np.random.randint(0, len(x_train))\n","        x_anchor = x_train[anchor_idx]\n","        y_anchor = y_train[anchor_idx]\n","        \n","        # Find the indices of samples with the same label as the anchor\n","        pos_indices = np.where(y_train == y_anchor)[0]\n","        # Select a random positive sample\n","        pos_idx = np.random.choice(pos_indices)\n","        x_positive = x_train[pos_idx]\n","        \n","        # Find the indices of samples with a different label than the anchor\n","        neg_indices = np.where(y_train != y_anchor)[0]\n","        # Select a random negative sample\n","        neg_idx = np.random.choice(neg_indices)\n","        x_negative = x_train[neg_idx]\n","        \n","        # Add the samples to the batch\n","        x_anchors[i] = x_anchor\n","        x_positives[i] = x_positive\n","        x_negatives[i] = x_negative\n","    \n","    return [x_anchors, x_positives, x_negatives]\n","\n","def triplet_loss(alpha):\n","    def loss(y_true, y_pred):\n","        anchor, positive, negative = tf.split(y_pred, 3, axis=1)\n","        positive_dist = tf.reduce_mean(tf.square(anchor - positive), axis=1)\n","        negative_dist = tf.reduce_mean(tf.square(anchor - negative), axis=1)\n","        return tf.maximum(positive_dist - negative_dist + alpha, 0.)\n","    return loss\n","\n","def build_siamese_model(num_of_features, emb_size):\n","    embedding_model = tf.keras.models.Sequential([\n","        tf.keras.layers.Dense(64, activation='relu', input_shape=(num_of_features,)),\n","        tf.keras.layers.Dense(128, activation='relu'),\n","        tf.keras.layers.Dense(256, activation='relu'),\n","        tf.keras.layers.Dense(512, activation='relu'),\n","        tf.keras.layers.Dense(1024, activation='relu'),\n","        #tf.keras.layers.Dropout(0.5),\n","        tf.keras.layers.Dense(2048, activation='relu'),\n","        #tf.keras.layers.Dropout(0.5),\n","        tf.keras.layers.Dense(4096, activation='relu'),\n","        #tf.keras.layers.Dropout(0.5),\n","        tf.keras.layers.Dense(emb_size, activation='sigmoid')\n","    ])\n","\n","\n","\n","    input_anchor = tf.keras.layers.Input(shape=(num_of_features,))\n","    input_positive = tf.keras.layers.Input(shape=(num_of_features,))\n","    input_negative = tf.keras.layers.Input(shape=(num_of_features,))\n","    \n","    embedding_anchor = embedding_model(input_anchor)\n","    embedding_positive = embedding_model(input_positive)\n","    embedding_negative = embedding_model(input_negative)\n","\n","    output = tf.keras.layers.concatenate([embedding_anchor, embedding_positive, embedding_negative], axis=1)\n","\n","    siamese_model = tf.keras.models.Model([input_anchor, input_positive, input_negative], output)\n","    \n","    return siamese_model\n","\n","def train_siamese_model(model, X_train, y_train, batch_size):\n","    data_gen = data_generator(batch_size,X_train,y_train)\n","    \n","    #optimizer=tf.keras.optimizers.Adam(learning_rate=0.001)\n","    model.compile(loss=triplet_loss(alpha=0.01), optimizer='adam',run_eagerly=True)\n","    history=model.fit(data_gen,\n","                      epochs=120,\n","                      steps_per_epoch=int(len(X_train)/batch_size),\n","                      verbose=2)\n","    \n","    loss = history.history['loss']\n","\n","    # Plot the training loss\n","    plt.plot(loss)\n","    plt.title('Training Loss')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.show()\n","    \n","   \n","\n","def data_generator(batch_size,x_dataset,y_dataset):\n","    while True:\n","        x=create_triplet_samples(x_dataset,y_dataset,num_of_features,batch_size)\n","        y=np.zeros((batch_size,num_of_features*3))\n","        \n","        yield x,y\n","\n","# Load and preprocess dataset\n","#X,y=load_and_preprocess_dataset(dataset_json['path_train_csv'])\n","\n","# Split into training and test sets\n","X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)\n","\n","# Reshape and normalize data\n","X_train=np.reshape(X_train,(X_train.shape[0],X_train.shape[1]))/255.\n","X_test=np.reshape(X_test,(X_test.shape[0],X_test.shape[1]))/255.\n","\n","# Build and train Siamese model\n","num_of_features=X_train.shape[1]\n","emb_size=64\n","siamese_model=build_siamese_model(num_of_features,emb_size)\n","train_siamese_model(siamese_model,X_train,y_train,batch_size=64)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-07T11:47:00.659026Z","iopub.status.busy":"2023-08-07T11:47:00.657997Z","iopub.status.idle":"2023-08-07T11:47:00.670336Z","shell.execute_reply":"2023-08-07T11:47:00.668842Z","shell.execute_reply.started":"2023-08-07T11:47:00.658981Z"},"trusted":true},"outputs":[],"source":["def create_test_triplets(x_test, y_test, n_features, n_batch):\n","    while True:\n","        x_anchors = np.zeros((n_batch, n_features))\n","        x_positives = np.zeros((n_batch, n_features))\n","        x_negatives = np.zeros((n_batch, n_features))\n","\n","        for i in range(n_batch):\n","            # Select a random sample from x_test as the anchor\n","            anchor_idx = np.random.randint(0, len(x_test))\n","            x_anchor = x_test[anchor_idx]\n","            y_anchor = y_test[anchor_idx]\n","\n","            # Find the indices of samples with the same label as the anchor\n","            pos_indices = np.where(y_test == y_anchor)[0]\n","            # Select a random positive sample\n","            pos_idx = np.random.choice(pos_indices)\n","            x_positive = x_test[pos_idx]\n","\n","            # Find the indices of samples with a different label than the anchor\n","            neg_indices = np.where(y_test != y_anchor)[0]\n","            # Select a random negative sample\n","            neg_idx = np.random.choice(neg_indices)\n","            x_negative = x_test[neg_idx]\n","\n","            # Add the samples to the batch\n","            x_anchors[i] = x_anchor\n","            x_positives[i] = x_positive\n","            x_negatives[i] = x_negative\n","\n","        yield [x_anchors, x_positives, x_negatives]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-08-07T10:43:48.986019Z","iopub.status.busy":"2023-08-07T10:43:48.985401Z","iopub.status.idle":"2023-08-07T10:43:50.216830Z","shell.execute_reply":"2023-08-07T10:43:50.215620Z","shell.execute_reply.started":"2023-08-07T10:43:48.985957Z"},"trusted":true},"outputs":[],"source":["from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","\n","def evaluate_siamese_model(model, X_test, y_test, batch_size):\n","    # Create a test data generator\n","    test_data_gen = create_test_triplets(X_test, y_test, num_of_features, batch_size)\n","    \n","    # Compute the embeddings for the test data\n","    n_batches = int(np.ceil(len(X_test) / batch_size))\n","    embeddings = []\n","    for i in range(n_batches):\n","        x_test_batch = next(test_data_gen)\n","        embeddings_batch = model.predict(x_test_batch)\n","        embeddings.append(embeddings_batch)\n","    embeddings = np.concatenate(embeddings)\n","    \n","    # Compute the pairwise distances between all embeddings\n","    n_embeddings = len(embeddings)\n","    dists = np.zeros((n_embeddings, n_embeddings))\n","    for i in range(n_embeddings):\n","        for j in range(n_embeddings):\n","            dists[i, j] = np.linalg.norm(embeddings[i] - embeddings[j])\n","    \n","    # Compute the predictions and true labels for all pairs of samples\n","    y_true = []\n","    y_pred = []\n","    for i in range(len(y_test)):\n","        for j in range(i+1, len(y_test)):\n","            if y_test[i] == y_test[j]:\n","                # Samples i and j have the same label\n","                y_true.append(1)\n","                y_pred.append(dists[i, j] < 0.5)\n","            else:\n","                # Samples i and j have different labels\n","                y_true.append(0)\n","                y_pred.append(dists[i, j] >= 0.5)\n","                \n","    \n","    # Compute the accuracy, precision, and F1 score\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred)\n","    f1 = f1_score(y_true, y_pred)\n","\n","    # Compute the confusion matrix\n","    cm = confusion_matrix(y_true, y_pred)\n","\n","    # Plot the confusion matrix\n","    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n","    disp.plot()\n","    \n","    return accuracy, precision, f1\n","\n","# Evaluate the Siamese model on the test data\n","accuracy, precision, f1 = evaluate_siamese_model(siamese_model, X_test, y_test,batch_size=64)\n","print(f'Test accuracy: {accuracy:.2f}')\n","print(f'Test precision: {precision:.2f}')\n","print(f'Test F1 score: {f1:.2f}')\n"]},{"cell_type":"markdown","metadata":{},"source":["# for classification (optional)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def build_embedding_model(num_of_features, emb_size):\n","    embedding_model = tf.keras.models.Sequential([\n","        tf.keras.layers.Dense(64, activation='relu', input_shape=(num_of_features,)),\n","        tf.keras.layers.Dense(emb_size, activation='sigmoid')\n","    ])\n","    \n","    return embedding_model\n","\n","def evaluate_siamese_model(model, embedding_model, X_test, y_test):\n","    # Compute the embeddings for the test set\n","    test_embeddings = embedding_model.predict(X_test)\n","    \n","    # Compute the pairwise distances between all embeddings\n","    dist_matrix = np.zeros((len(test_embeddings), len(test_embeddings)))\n","    for i in range(len(test_embeddings)):\n","        for j in range(len(test_embeddings)):\n","            dist_matrix[i,j] = np.linalg.norm(test_embeddings[i] - test_embeddings[j])\n","    \n","    # Compute the average distance between embeddings of the same class\n","    same_class_dist = []\n","    for i in range(len(test_embeddings)):\n","        same_class_indices = np.where(y_test == y_test[i])[0]\n","        same_class_dist.append(np.mean(dist_matrix[i,same_class_indices]))\n","    same_class_dist = np.mean(same_class_dist)\n","    \n","    # Compute the average distance between embeddings of different classes\n","    diff_class_dist = []\n","    for i in range(len(test_embeddings)):\n","        diff_class_indices = np.where(y_test != y_test[i])[0]\n","        diff_class_dist.append(np.mean(dist_matrix[i,diff_class_indices]))\n","    diff_class_dist = np.mean(diff_class_dist)\n","    \n","    print(f'Average distance between embeddings of the same class: {same_class_dist:.4f}')\n","    print(f'Average distance between embeddings of different classes: {diff_class_dist:.4f}')\n","\n","# Build embedding model\n","embedding_model = build_embedding_model(num_of_features, emb_size)\n","\n","# Set weights of embedding model to be the same as the Siamese model\n","embedding_model.set_weights(siamese_model.layers[3].get_weights())\n","\n","# Evaluate Siamese model on test set\n","evaluate_siamese_model(siamese_model, embedding_model, X_test, y_test)\n"]},{"cell_type":"markdown","metadata":{},"source":["# Visualization using t-SNE\n","\n","However, it’s important to note that this Siamese network model is an unsupervised learning model that   embed data points into a lower-dimensional space such that similar data points are close together and dissimilar data points are far apart. As such, traditional classification metrics like accuracy, precision, recall, and F1 score are not applicable to this type of model.\n","\n","Instead, we use other methods to evaluate the performance of the model, such as visualizing the embeddings using dimensionality reduction techniques like t-SNE or UMAP. Here we created a 2D visualization of the embeddings using t-SNE:"]},{"cell_type":"markdown","metadata":{},"source":["t-SNE (t-Distributed Stochastic Neighbor Embedding) is a nonlinear dimensionality reduction technique that is particularly effective at preserving the local structure of the data. It works by minimizing the divergence between two distributions: a distribution that measures pairwise similarities of the input objects and a distribution that measures pairwise similarities of the corresponding low-dimensional points in the embedding1."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.manifold import TSNE\n","import matplotlib.pyplot as plt\n","\n","def visualize_embeddings(embedding_model, X_test, y_test):\n","    # Compute the embeddings for the test set\n","    test_embeddings = embedding_model.predict(X_test)\n","    \n","    # Reduce the dimensionality of the embeddings to 2D using t-SNE\n","    tsne = TSNE(n_components=2,perplexity=10)\n","    tsne_embeddings = tsne.fit_transform(test_embeddings)\n","    \n","    # Create a scatter plot of the 2D embeddings\n","    plt.scatter(tsne_embeddings[:,0], tsne_embeddings[:,1], c=y_test, cmap='tab10')\n","    plt.colorbar()\n","    plt.show()\n","\n","# Visualize embeddings of test set\n","visualize_embeddings(embedding_model, X_test, y_test)\n"]},{"cell_type":"markdown","metadata":{},"source":["This code computes the embeddings for the test set using the predict method of the embedding_model, then reduces the dimensionality of the embeddings to 2D using t-SNE. Finally, it creates a scatter plot of the 2D embeddings, where each point is colored according to its class label.\n","\n","This scatter plot shows the 2D visualization of the embeddings of our data points using the t-SNE technique. The plot has four distinct clusters of data points, each represented by a different color. The color scale on the right side of the plot ranges from 1 to 10, indicating that there are 10 different classes in our data.\n","\n","The t-SNE technique is designed to preserve the local structure of the data, so data points that are close together in the high-dimensional space are likely to be close together in the 2D visualization as well. From the plot, we can see that the four clusters are well-separated from each other, indicating that the Siamese network model is able to learn a good representation of the data that separates the different classes."]},{"cell_type":"markdown","metadata":{},"source":["# Using PCA visulaization\n","This code computes the embeddings for the test set using the predict method of the embedding_model, then reduces the dimensionality of the embeddings to 2D using PCA. Finally, it creates a scatter plot of the 2D embeddings, where each point is colored according to its class label."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from sklearn.decomposition import PCA\n","import matplotlib.pyplot as plt\n","\n","def visualize_embeddings_pca(embedding_model, X_test, y_test):\n","    # Compute the embeddings for the test set\n","    test_embeddings = embedding_model.predict(X_test)\n","    \n","    # Reduce the dimensionality of the embeddings to 2D using PCA\n","    pca = PCA(n_components=2)\n","    pca_embeddings = pca.fit_transform(test_embeddings)\n","    \n","    # Create a scatter plot of the 2D embeddings\n","    plt.scatter(pca_embeddings[:,0], pca_embeddings[:,1], c=y_test, cmap='tab10')\n","    plt.colorbar()\n","    plt.show()\n","\n","# Visualize embeddings of test set using PCA\n","visualize_embeddings_pca(embedding_model, X_test, y_test)\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.17"},"vscode":{"interpreter":{"hash":"f4ce3f0b7eb266cd73545e7d6554a6d43a6324250d8a1233df0fc87ecac26317"}}},"nbformat":4,"nbformat_minor":4}
